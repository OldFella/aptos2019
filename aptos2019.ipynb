{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aptos2019.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfFtU98waNSm",
        "colab_type": "text"
      },
      "source": [
        "# Mount G-Drive:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsXCTYKgaMQK",
        "colab_type": "code",
        "outputId": "f15e104d-8959-41d0-e162-936d304484e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLYhXzYGarkk",
        "colab_type": "text"
      },
      "source": [
        "# Config:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPwzvCKHataI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = '/content/gdrive/My Drive/aptos2019/'\n",
        "\n",
        "MODEL_PATH = DATA_PATH + \"models/\"\n",
        "TRAIN_PATH = DATA_PATH + \"train/\"\n",
        "VALIDATION_PATH = DATA_PATH + \"validation/\"\n",
        "EVEN_TRAIN_PATH = DATA_PATH + \"train_even/\"\n",
        "ADD_EVEN_TRAIN_PATH = DATA_PATH + \"add_train_even/\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZP3FC0UakGy",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S99-238Sanmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "\ttransforms.Resize((350,350)),\n",
        "\ttransforms.RandomVerticalFlip(0.5),\n",
        "\ttransforms.RandomHorizontalFlip(0.5),\n",
        "\ttransforms.RandomRotation(360),\n",
        "\ttransforms.ToTensor(),\n",
        "\tnormalize\n",
        "\t])\n",
        "\n",
        "validation_transforms = transforms.Compose([\n",
        "\ttransforms.Resize((350,350)),\n",
        "\ttransforms.ToTensor(),\n",
        "\tnormalize\n",
        "\t])\n",
        "\n",
        "\n",
        "def create_loader(folder, transforms, batch_size = 4):\n",
        "\tdataset = torchvision.datasets.ImageFolder(\n",
        "\t\troot = DATA_PATH + folder,\n",
        "\t\ttransform = transforms)\n",
        "\t\n",
        "\tloader = torch.utils.data.DataLoader(\n",
        "\t\tdataset,\n",
        "\t\tbatch_size = batch_size,\n",
        "\t\tshuffle = True,\n",
        "\t\tnum_workers = 1)\n",
        "\n",
        "\treturn loader\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW8UwZ6qcJks",
        "colab_type": "text"
      },
      "source": [
        "# test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzDkPLH_cLJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "test_loader = create_loader('validation/', validation_transforms)\n",
        "\n",
        "def test(test_loader, model, device = device):\n",
        "    \"\"\" \n",
        "    Loads the test data set and tests the model on it. \n",
        "    Returns achieved loss, correctly guessed samples and accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction = 'sum').item()\n",
        "            pred = output.argmax(dim = 1, keepdim = True)\n",
        "            \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        accuracy_percent = 100 * correct/len(test_loader.dataset)\n",
        "        print('\\nTest set: Average loss:  {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), accuracy_percent))\n",
        "    return test_loss, correct, len(test_loader.dataset), accuracy_percent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAi7tqEbjDyE",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW2IOU8OjFML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "def progress(count, total, epoch, suffix = ''):\n",
        "\t\"\"\" Writes a progress bar to console. \"\"\"\n",
        "\tbar_len = 40\n",
        "\tfilled_len = int(round(bar_len * count / float(total)))\n",
        "\n",
        "\tpercents = round(100.0 * count / float(total), 1)\n",
        "\tbar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
        "\t# sys.stdout.write('epoch %s:\\r' %(epoch))\n",
        "\tsys.stdout.write('epoch %s: [%s] %s%s ...%s\\r' % (epoch, bar, percents, '%', suffix))\n",
        "\t# sys.stdout.write('average_loss: %s\\r' % (average_loss))\n",
        "\tsys.stdout.flush()  # As suggested by Rom Ruben"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQhDsTuRb2Bd",
        "colab_type": "text"
      },
      "source": [
        "# train:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dalKfIIUb68A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "import torchvision\n",
        "import copy\n",
        "import numpy as np\n",
        "# import visualize\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "def train(model = None, training_data_path = \"train/\", criterion = None, training_epochs = 4, batch_size = 32, learning_rate = 0.001):\n",
        "    \"\"\" \n",
        "    Runs through the training data, makes a prediction and computes loss, then backpropagates\n",
        "    the result through the model and adjusts the weights and biases until a local minimum in the loss\n",
        "    function is reached.\n",
        "    \"\"\"\n",
        "\n",
        "    # optimizer searches fo a local minimum of in the lossfunction with different input parameters\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 0.03)\n",
        "    graph_loss = []\n",
        "    graph_accuracy = [(0,0)]\n",
        "    graph_validation_loss = []\n",
        "    validation_loader = create_loader(\"validation/\", validation_transforms, batch_size = batch_size)\n",
        "    best_model = None\n",
        "\n",
        "    threshold = 0\n",
        "    for epoch in range(training_epochs):\n",
        "        print('epoch: ', epoch + 1)\n",
        "        running_loss = 0.0\n",
        "\n",
        "        training_loader = create_loader(training_data_path, train_transforms, batch_size = batch_size)\n",
        "        average_loss = 0\n",
        "        print('')\n",
        "        number_of_files = len(training_loader.dataset)\n",
        "        for i, data in enumerate(training_loader, 0):\n",
        "\n",
        "            # get input for training\n",
        "            inputs, labels = data\n",
        "            \n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # init optimizer with 0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # run data trough net\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # compute loss (compare output to label)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # backpropagate loss\n",
        "            loss.backward()\n",
        "\n",
        "            # tweak parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # add loss to overall loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # pretty print progress\n",
        "            if i % 10 == 9:  # append a the average of the last 10 losses as point to the loss/epoch graph_loss\n",
        "                average_loss = running_loss/10\n",
        "                graph_loss.append((epoch + i/(number_of_files/batch_size), average_loss))\n",
        "                running_loss = 0.0\n",
        "                print(i,(number_of_files/batch_size),'loss:', average_loss)\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "        model_name = 'epoch{}.pt'.format(epoch)\n",
        "        # torch.save(best_model, MODEL_PATH + model_name)\n",
        "        #print(\"\\nmodel: \" + model_name + \" has been saved.\")\n",
        "        # Validate the result of the epoch\n",
        "        test_loss, correct, dataset_size, accuracy_percent = test(validation_loader, model)\n",
        "        graph_accuracy.append((epoch + 1, accuracy_percent/100))\n",
        "        graph_validation_loss.append((epoch + 1, test_loss))\n",
        "\n",
        "        \n",
        "    return model, model_name # return namedtuple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHTU_JIjcfCY",
        "colab_type": "text"
      },
      "source": [
        "# Main:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqoT5YxEch7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from torch import nn   \n",
        "from datetime import datetime\n",
        "import torch\n",
        "import argparse\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# init model\n",
        "Dense_NET = models.resnext101_32x8d(pretrained=True, progress=False)\n",
        "Dense_NET.cuda()\n",
        "\n",
        "\n",
        "# start timer\n",
        "start = datetime.now()\n",
        "print(start)\n",
        "\n",
        "EPOCHS_STAGE_1 = 1\n",
        "EPOCHS_STAGE_2 = 1\n",
        "LR_STAGE_1 = 0.001\n",
        "LR_STAGE_2 = 0.001\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "# train on long tailed training set\n",
        "trained_on_long_tailed_dataset = train(\n",
        "    model = Dense_NET,\n",
        "    training_data_path = \"train/\",\n",
        "    training_epochs = EPOCHS_STAGE_1,\n",
        "    criterion = nn.CrossEntropyLoss(),\n",
        "    learning_rate = LR_STAGE_1,\n",
        "    batch_size = BATCH_SIZE)\n",
        "\n",
        "\n",
        "# save models stage_dict after first stage\n",
        "torch.save(trained_on_long_tailed_dataset[0].state_dict(), MODEL_PATH + \"long_tailed_\" + trained_on_long_tailed_dataset[1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# use the model from first stage and load it with the saved stage dict  \n",
        "model = trained_on_long_tailed_dataset[0]\n",
        "model.load_state_dict(torch.load(MODEL_PATH + \"long_tailed_\" + trained_on_long_tailed_dataset[1]))\n",
        "\n",
        "\n",
        "# now, do finetuning on the evenly distributed training set\n",
        "finetuned_model = train(\n",
        "\tmodel = Dense_NET,\n",
        "\ttraining_data_path = \"add_train_even/\",\n",
        "\ttraining_epochs = EPOCHS_STAGE_2,\n",
        "\tcriterion = nn.CrossEntropyLoss(),\t\n",
        "\tlearning_rate = LR_STAGE_2,\n",
        "\tbatch_size = BATCH_SIZE)\n",
        "\n",
        "\n",
        "torch.save(finetuned_model[0].state_dict(), MODEL_PATH + \"finetuned_\" + finetuned_model[1])\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nOverall training and testing time: \" + str(datetime.now() - start))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}